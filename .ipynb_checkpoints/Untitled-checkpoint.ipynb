{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footy Tipping with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in training data\n",
    "data_train = pd.read_csv('afl_train.csv')\n",
    "# drop if year 2005\n",
    "data_train = data_train.drop(data_train[data_train.season==2000].index)\n",
    "data_train.fillna(0)\n",
    "# make X, y variables\n",
    "X = data_train[['home_percentage', 'away_percentage', \n",
    "                       'home_last_season_percentage', 'away_last_season_percentage']].as_matrix()\n",
    "y = data_train[['home_team_win']].as_matrix()\n",
    "m, n = X.shape\n",
    "X_cols = ['home_percentage', 'away_percentage', \n",
    "                       'home_last_season_percentage', 'away_last_season_percentage']\n",
    "y_col = ['home_team_win']\n",
    "\n",
    "# load in cross validation data\n",
    "data_train = pd.read_csv('afl_cval.csv')\n",
    "X_cv = data_train[['home_percentage', 'away_percentage', \n",
    "                       'home_last_season_percentage', 'away_last_season_percentage']].as_matrix()\n",
    "y_cv = data_train[['home_team_win']].as_matrix()\n",
    "m_cv, n_cv = X.shape\n",
    "X_cols_cv = ['home_percentage', 'away_percentage', \n",
    "                       'home_last_season_percentage', 'away_last_season_percentage']\n",
    "y_col_cv = ['home_team_win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN_3_layer:\n",
    "    \n",
    "    def __init__(self, X, y, hidden_layer_size, lamda, eps_init):\n",
    "        \"\"\"\n",
    "        Initialises the class NN_3_layer, \n",
    "        which computes a neural network with one hidden layer.\n",
    "        X is a m, input_layer_size size numpy array\n",
    "        Y is a m, output_layer_size size numpy array\n",
    "        m is the number of observations/training sets\n",
    "        input_layer_size is the number of features\n",
    "        output_layer_size is the size of the output layer\n",
    "        hidden_layer_size is the size of the hidden layer\n",
    "        lamda is the regularisation constant\n",
    "        eps_init is a constant initialising the initial theta\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lamda = lamda\n",
    "        self.eps_init = eps_init\n",
    "        self.m = X.shape[0]\n",
    "        self.input_layer_size = X.shape[1]\n",
    "        self.output_layer_size = y.shape[1]\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid function\n",
    "        \"\"\"\n",
    "        denominator = 1.0 + np.exp(-1.0 * z)\n",
    "        return 1.0 / denominator\n",
    "    \n",
    "    def sigmoid_gradient(self, a):\n",
    "        \"\"\"\n",
    "        computes the gradient of the sigmoid function at input value z\n",
    "        \"\"\"\n",
    "        a = np.array(a)\n",
    "        output_shape = a.shape\n",
    "        output = np.array([a * (1 - a)])\n",
    "        output.reshape(output_shape)\n",
    "        return output\n",
    "    \n",
    "    def square_up(self, theta_vector):\n",
    "        \"\"\"\n",
    "        Returns the theta vector to matrices\n",
    "        \"\"\"\n",
    "        vector_1_length = (self.input_layer_size + 1) * self.hidden_layer_size\n",
    "        matrix_1 = theta_vector[0:vector_1_length].reshape(\n",
    "            (self.hidden_layer_size, self.input_layer_size + 1))\n",
    "        matrix_2 = theta_vector[vector_1_length:]\n",
    "        matrix_2 = matrix_2.reshape(\n",
    "            (self.output_layer_size, self.hidden_layer_size + 1))\n",
    "        return (matrix_1, matrix_2)        \n",
    "\n",
    "    def flatten_out(self, thetas):\n",
    "        \"\"\"\n",
    "        Converts features from matrices to vectors.\n",
    "        \"\"\"\n",
    "        return np.hstack((thetas[0].reshape(-1), thetas[1].reshape(-1)))\n",
    "\n",
    "    def forward_propogation(self, thetas, x):\n",
    "        \"\"\"\n",
    "        Forward propogation for a three layer nn\n",
    "        Inputs:\n",
    "            i = input layer\n",
    "            h = hidden layer\n",
    "            o = output layer (number of classes)\n",
    "            m = number of training sets / observations\n",
    "            theta1: (i+1) x h numpy array\n",
    "            theta2: (h+1) x o numpy array\n",
    "            X: m x i numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        (self.theta1, self.theta2) = self.square_up(thetas)\n",
    "        m_fp = x.shape[0]\n",
    "        # input layer\n",
    "        a1 = np.ones((m_fp, self.input_layer_size+1))\n",
    "        a1[:,1:] = x\n",
    "        # hidden Layer\n",
    "        z2 = np.dot(self.theta1, a1.T)\n",
    "        a2_0 = self.sigmoid(z2)\n",
    "        a2 = np.ones((a2_0.shape[0]+1, m_fp))\n",
    "        a2[1:,:] = a2_0\n",
    "        # output layer\n",
    "        z3 = np.dot(self.theta2, a2)\n",
    "        a3 = self.sigmoid(z3)    \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def cost_function(self, thetas):\n",
    "        \"\"\"\n",
    "        Calculates the cost function J after a round of forward propogation\n",
    "        Inputs\n",
    "            i = input layer\n",
    "            h = hidden layer\n",
    "            o = output layer (number of classes)\n",
    "            m = number of training sets / observations\n",
    "            theta1: (i+1) x h numpy array\n",
    "            theta2: (h+1) x o numpy array\n",
    "            X: m x i numpy array\n",
    "        Output is a float\n",
    "        \"\"\"\n",
    "        a1, z1, a2, z2, a3 = self.forward_propogation(thetas, self.X)\n",
    "        h = a3\n",
    "        j1 = np.dot(np.log(h), self.y)\n",
    "        j2 = np.dot(np.log(1 - h), (1-self.y))\n",
    "        J = (-1./len(y)) * (j1+j2)\n",
    "        J = J[0][0]\n",
    "        return J\n",
    "    \n",
    "    def cost_function_reg(self, thetas):\n",
    "        \"\"\"\n",
    "        Calculates the regularized cost function J after a round of forward propogation\n",
    "        Inputs\n",
    "            thetas are rolled feature spaces\n",
    "            lamda is a float\n",
    "            i = input layer\n",
    "            h = hidden layer\n",
    "            o = output layer (number of classes)\n",
    "            m = number of training sets / observations\n",
    "            theta1: (i+1) x h numpy array\n",
    "            theta2: (h+1) x o numpy array\n",
    "            X: m x i numpy array\n",
    "        Output is a float\n",
    "        \"\"\"\n",
    "        base = self.cost_function(thetas)\n",
    "        reg_0 = self.lamda / float(2*self.m)\n",
    "        theta_flat = self.flatten_out(thetas)\n",
    "        reg_1 = sum([t**2 for t in theta_flat])\n",
    "        reg_term = reg_0 * reg_1\n",
    "        return base + reg_term\n",
    "\n",
    "    def cost_gradient(self, thetas):\n",
    "        \"\"\"\n",
    "        Approximates the gradient vector of the NN via backpropagation\n",
    "        \"\"\"\n",
    "        (theta1, theta2) = self.square_up(thetas) \n",
    "        init1_shape = (self.hidden_layer_size, self.input_layer_size+1)\n",
    "        init2_shape = (self.output_layer_size, self.hidden_layer_size+1)\n",
    "        delta_1 = np.zeros(init1_shape)\n",
    "        delta_2 = np.zeros(init2_shape)\n",
    "        count = 0\n",
    "        ## back propagation\n",
    "        for obs in self.X:\n",
    "            a1, z2, a2, z3, a3 = self.forward_propogation(thetas,\n",
    "                                obs.reshape(1, self.input_layer_size))\n",
    "            # layer three\n",
    "            # layer three\n",
    "            delta_3_k = (a3 - self.y[count])[0][0]\n",
    "            # layer two\n",
    "            term1 = np.dot(theta2.T, delta_3_k) \n",
    "            term2 = self.sigmoid_gradient(a2)[0]\n",
    "            delta_2_k = term1 * term2\n",
    "            delta_2_k = delta_2_k[1:]\n",
    "            # calculating delta terms\n",
    "            term_2_ij = np.dot(delta_3_k, a2.T)\n",
    "            term_1_ij = np.dot(delta_2_k, a1)\n",
    "            delta_2 = delta_2 + term_2_ij\n",
    "            delta_1 = delta_1 + term_1_ij\n",
    "            count+=1\n",
    "\n",
    "        delta_1 = delta_1 / float(m)\n",
    "        delta_2 = delta_2 / float(m)\n",
    "        deltas = self.flatten_out((delta_1, delta_2))\n",
    "        return deltas\n",
    "    \n",
    "    def cost_gradient_reg(self, thetas):\n",
    "        \"\"\"\n",
    "        Approximates the regularised gradient vector of the NN via backpropagation\n",
    "\n",
    "        \"\"\"\n",
    "        (theta1, theta2) = self.square_up(thetas)\n",
    "        grad = self.cost_gradient(thetas)\n",
    "        grad = self.square_up(grad)\n",
    "        term1 = self.lamda / self.m\n",
    "        t1 = np.zeros(theta1.shape)\n",
    "        t1[:,1:] = self.theta1[:,1:]\n",
    "        t2 = np.zeros(theta2.shape)\n",
    "        t2[:,1:] = self.theta2[:,1:]\n",
    "        grad1 = grad[0] + (term1 * t1)\n",
    "        grad2 = grad[1] + (term1 * t2)\n",
    "        grad = self.flatten_out((grad1, grad2))\n",
    "        return grad\n",
    "    \n",
    "    def numerical_gradient(self):\n",
    "        \"\"\"\n",
    "        Approximates the gradient of the cost function by perturbing \n",
    "        element ij of layer ell by eps\n",
    "        \"\"\"\n",
    "        thetas = flatten_out((theta1, theta2))\n",
    "        f = []\n",
    "        eps_g = 1e-3\n",
    "        for i in range(len(self.theta_flat)):\n",
    "            print \"Element\", i, \"of\", len(self.theta_flat)\n",
    "            theta_high = self.theta_flat\n",
    "            theta_low = self.theta_flat\n",
    "            theta_high[i] = theta_high[i] + eps_g\n",
    "            theta_low[i] = theta_low[i] - eps_g\n",
    "            j_high = self.cost_function_reg(theta_high)\n",
    "            j_low = self.cost_function_reg(theta_low)\n",
    "            f_theta_approx = (j_high - j_low) / (2 * eps_g)\n",
    "            f.append(f_theta_approx[0])\n",
    "        return np.array(f)\n",
    "    \n",
    "    def prediction(self, a, y):\n",
    "        pr = np.argmax(a.T, axis=1).reshape(y.shape)\n",
    "        accuracy = 100 * sum((pr==y)) / float(len(y))\n",
    "        print 'The neural network correctly predicts %f percent of the cells' % accuracy\n",
    "        return\n",
    "    \n",
    "    def theta_init(self):\n",
    "        \"Initialises the theta\"\n",
    "        self.theta1 = np.random.rand(self.hidden_layer_size, self.input_layer_size+1\n",
    "                              ) * 2 * self.eps_init - self.eps_init\n",
    "        self.theta2 = np.random.rand(self.output_layer_size, self.hidden_layer_size+1\n",
    "                              ) * 2 * self.eps_init - self.eps_init\n",
    "        self.thetas_flat = self.flatten_out((self.theta1, self.theta2))\n",
    "    \n",
    "    def opt_finder(self):\n",
    "        self.theta_init()\n",
    "        self.theta_opt = minimize(\n",
    "            self.cost_function_reg, x0=self.thetas_flat, \n",
    "            method=\"TNC\", jac=self.cost_gradient_reg, \n",
    "            options={\"maxiter\":500, \"disp\":True}).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NN_3_layer(X, y, 4, 0, 0.12)\n",
    "nn.opt_finder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network correctly predicts 42.045455 percent of the cells\n"
     ]
    }
   ],
   "source": [
    "a1, z2, a2, z3, a3 = nn.forward_propogation(nn.theta_opt, X)\n",
    "nn.prediction(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
